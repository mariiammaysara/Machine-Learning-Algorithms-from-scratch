{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest â€” From Scratch\n\nIn this notebook, we implement the **Random Forest Classifier** completely from scratch using only **NumPy**.\n\nWe will:\n- Generate a synthetic dataset  \n- Build individual decision trees  \n- Implement feature bagging  \n- Implement bootstrap sampling  \n- Aggregate predictions using majority vote  \n- Visualize the decision boundary  \n- Evaluate model accuracy  \n\nRandom Forest is an ensemble learning method that combines multiple decision trees to improve stability and accuracy while reducing overfitting.\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## What is Random Forest?\n\nRandom Forest is an ensemble method that builds multiple decision trees using:\n\n1. **Bootstrap Sampling**  \n   Each tree is trained on a random sample (with replacement) of the training dataset.\n\n2. **Random Feature Selection**  \n   At every split, the tree evaluates **a random subset of features**, not all features.\n\nEach tree is slightly different, and predictions are aggregated using **majority vote**:\n\n$$\n\\hat{y} = \\text{mode} \\left( \\{ h_1(x), h_2(x), \\dots, h_T(x) \\} \\right)\n$$\n\nWhere:\n- $( h_t(x) )$ = prediction of tree \\( t \\)  \n- $( T )$ = number of trees  \n\nThis reduces variance and improves generalization compared to a single decision tree.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:35:05.617558Z","iopub.execute_input":"2025-11-27T12:35:05.618161Z","iopub.status.idle":"2025-11-27T12:35:06.202446Z","shell.execute_reply.started":"2025-11-27T12:35:05.618130Z","shell.execute_reply":"2025-11-27T12:35:06.201652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Decision Tree Implementation (simplified for RF)","metadata":{}},{"cell_type":"code","source":"class DecisionTree:\n\n    def __init__(self, max_depth=5, min_samples_split=2, num_features=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.num_features = num_features\n        self.tree = None\n\n    def fit(self, X, y):\n        self.n_features = X.shape[1]\n        self.num_features = self.num_features or self.n_features\n        self.tree = self._build(X, y, depth=0)\n\n    def _gini(self, y):\n        probs = [np.mean(y == c) for c in np.unique(y)]\n        return 1 - sum(p*p for p in probs)\n\n    def _best_split(self, X, y):\n        best_feature, best_threshold, best_gini = None, None, 1e9\n        feature_indices = np.random.choice(self.n_features, self.num_features, replace=False)\n\n        for feature in feature_indices:\n            values = np.unique(X[:, feature])\n            for t in values:\n                left = y[X[:, feature] <= t]\n                right = y[X[:, feature] > t]\n                if len(left) == 0 or len(right) == 0:\n                    continue\n\n                gini = (len(left)*self._gini(left) + len(right)*self._gini(right)) / len(y)\n\n                if gini < best_gini:\n                    best_gini = gini\n                    best_feature = feature\n                    best_threshold = t\n\n        return best_feature, best_threshold\n\n    def _build(self, X, y, depth):\n        if depth >= self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:\n            return np.argmax(np.bincount(y))\n\n        feature, threshold = self._best_split(X, y)\n        if feature is None:\n            return np.argmax(np.bincount(y))\n\n        left_idx = X[:, feature] <= threshold\n        right_idx = ~left_idx\n\n        return {\n            \"feature\": feature,\n            \"threshold\": threshold,\n            \"left\": self._build(X[left_idx], y[left_idx], depth+1),\n            \"right\": self._build(X[right_idx], y[right_idx], depth+1),\n        }\n\n    def _predict_one(self, x, node):\n        if not isinstance(node, dict):\n            return node\n\n        if x[node[\"feature\"]] <= node[\"threshold\"]:\n            return self._predict_one(x, node[\"left\"])\n        else:\n            return self._predict_one(x, node[\"right\"])\n\n    def predict(self, X):\n        return np.array([self._predict_one(x, self.tree) for x in X])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:35:49.688434Z","iopub.execute_input":"2025-11-27T12:35:49.688904Z","iopub.status.idle":"2025-11-27T12:35:49.703029Z","shell.execute_reply.started":"2025-11-27T12:35:49.688877Z","shell.execute_reply":"2025-11-27T12:35:49.702112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Random Forest Implementation","metadata":{}},{"cell_type":"code","source":"class RandomForest:\n\n    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, num_features=None):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.num_features = num_features\n        self.trees = []\n\n    def _bootstrap(self, X, y):\n        idx = np.random.choice(len(X), len(X), replace=True)\n        return X[idx], y[idx]\n\n    def fit(self, X, y):\n        self.trees = []\n        for _ in range(self.n_trees):\n            X_sample, y_sample = self._bootstrap(X, y)\n            tree = DecisionTree(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                num_features=self.num_features\n            )\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        return np.apply_along_axis(lambda row: np.bincount(row).argmax(), axis=0, arr=tree_preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:36:46.894559Z","iopub.execute_input":"2025-11-27T12:36:46.894916Z","iopub.status.idle":"2025-11-27T12:36:46.902591Z","shell.execute_reply.started":"2025-11-27T12:36:46.894889Z","shell.execute_reply":"2025-11-27T12:36:46.901662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Generate Dataset","metadata":{}},{"cell_type":"code","source":"np.random.seed(42)\n\nX, y = make_classification(\n    n_samples=300,\n    n_features=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    class_sep=1.7\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:37:33.691684Z","iopub.execute_input":"2025-11-27T12:37:33.692139Z","iopub.status.idle":"2025-11-27T12:37:33.707476Z","shell.execute_reply.started":"2025-11-27T12:37:33.692112Z","shell.execute_reply":"2025-11-27T12:37:33.706582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Train the Random Forest","metadata":{}},{"cell_type":"code","source":"rf = RandomForest(n_trees=15, max_depth=6, num_features=1)\nrf.fit(X, y)\ny_pred = rf.predict(X)\n\nprint(\"Accuracy:\", np.mean(y_pred == y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:38:14.121119Z","iopub.execute_input":"2025-11-27T12:38:14.121786Z","iopub.status.idle":"2025-11-27T12:38:14.496686Z","shell.execute_reply.started":"2025-11-27T12:38:14.121753Z","shell.execute_reply":"2025-11-27T12:38:14.495920Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Decision Boundary Visualization","metadata":{}},{"cell_type":"code","source":"x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\ny_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\ngrid = np.c_[xx.ravel(), yy.ravel()]\nzz = rf.predict(grid).reshape(xx.shape)\n\nplt.contourf(xx, yy, zz, cmap=\"bwr\", alpha=0.3)\nplt.scatter(X[:,0], X[:,1], c=y, cmap=\"bwr\")\nplt.title(\"Random Forest Decision Boundary\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:38:46.430192Z","iopub.execute_input":"2025-11-27T12:38:46.430820Z","iopub.status.idle":"2025-11-27T12:38:48.123113Z","shell.execute_reply.started":"2025-11-27T12:38:46.430792Z","shell.execute_reply":"2025-11-27T12:38:48.122234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation\n\n- **Accuracy:** 1.0  \n- The Random Forest achieves perfect performance on this dataset.  \n- The decision boundary is non-linear and composed of many small, piecewise regions created by multiple trees.  \n- Because each tree is trained on a bootstrap sample with random features, the ensemble reduces overfitting compared to a single decision tree.  \n- Majority voting produces stable and reliable predictions.  \n- The method captures complex class boundaries that a single tree cannot model alone.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
